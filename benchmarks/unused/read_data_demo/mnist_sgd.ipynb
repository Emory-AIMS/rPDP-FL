{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 6000 samples\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "import math\n",
    "from fedrpdp.accountants.rdp import RDPAccountant\n",
    "from fedrpdp.record_budget_generation import EPSILONS_GEN_FUNC\n",
    "from fedrpdp.autodp.optimizers.pdp_optimizer import make_optimizer_class as pdp_optim_cls\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, device, train_loader, optimizer, epoch, running_norms, disable_dp=False, delta=1e-5):\n",
    "#     model.train()\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     losses = []\n",
    "\n",
    "#     data, target = next(iter(train_loader))\n",
    "#     correct = 0\n",
    "#     data, target = data.to(device), target.to(device)\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(data)\n",
    "\n",
    "#     # compute train acc\n",
    "#     pred = output.argmax(\n",
    "#             dim=1, keepdim=True\n",
    "#         )  # get the index of the max log-probability\n",
    "#     correct = pred.eq(target.view_as(pred)).sum().item() \n",
    "\n",
    "#     loss = criterion(output, target)\n",
    "#     loss.backward()\n",
    "#     # gradient_norms is a Tensor with size being equal to `dataset_size`\n",
    "#     gradient_norms = optimizer.step(running_norms)\n",
    "#     gradient_norms_sq = gradient_norms * gradient_norms\n",
    "#     losses.append(loss.item())\n",
    "\n",
    "#     # for data, target in train_loader:\n",
    "#     #     correct = 0\n",
    "#     #     data, target = data.to(device), target.to(device)\n",
    "#     #     optimizer.zero_grad()\n",
    "#     #     output = model(data)\n",
    "\n",
    "#     #     # compute train acc\n",
    "#     #     pred = output.argmax(\n",
    "#     #             dim=1, keepdim=True\n",
    "#     #         )  # get the index of the max log-probability\n",
    "#     #     correct = pred.eq(target.view_as(pred)).sum().item() \n",
    "\n",
    "#     #     loss = criterion(output, target)\n",
    "#     #     loss.backward()\n",
    "#     #     gradient_norms = optimizer.step(running_norms)\n",
    "#     #     gradient_norms_sq = gradient_norms * gradient_norms\n",
    "#     #     losses.append(loss.item())\n",
    "\n",
    "#     if not disable_dp:\n",
    "#         # Note that we only show the 1st point's cumulative privacy cost.\n",
    "#         epsilon_1, best_alpha_1 = optimizer.privacy_engine.get_privacy_spent(0, delta)\n",
    "#         epsilon_2, best_alpha_2 = optimizer.privacy_engine.get_privacy_spent(40000, delta)\n",
    "#         epsilon_3, best_alpha_3 = optimizer.privacy_engine.get_privacy_spent(50000, delta)\n",
    "#         print(\n",
    "#             f\"Train Epoch: {epoch} \\t\"\n",
    "#             f\"Loss: {np.mean(losses):.6f} \"\n",
    "#             f\"Acc: {correct/60000.0:.6f} \"\n",
    "#             f\"δ: {delta} \"\n",
    "#             f\"ε1 = {epsilon_1:.2f} for α1 = {best_alpha_1}, \"\n",
    "#             f\"ε2 = {epsilon_2:.2f} for α2 = {best_alpha_2}, \"\n",
    "#             f\"ε3 = {epsilon_3:.2f} for α3 = {best_alpha_3}. \"\n",
    "#         )\n",
    "#     else:\n",
    "#         print(f\"Train Epoch: {epoch} \\t Loss: {np.mean(losses):.6f}\")\n",
    "\n",
    "#     return gradient_norms_sq\n",
    "\n",
    "\n",
    "# def test(model, device, test_loader):\n",
    "#     model.eval()\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     test_loss = 0\n",
    "#     correct = 0\n",
    "#     with torch.no_grad():\n",
    "#         for data, target in test_loader:\n",
    "#             data, target = data.to(device), target.to(device)\n",
    "#             output = model(data)\n",
    "#             test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "#             pred = output.argmax(\n",
    "#                 dim=1, keepdim=True\n",
    "#             )  # get the index of the max log-probability\n",
    "#             correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "#     test_loss /= len(test_loader.dataset)\n",
    "\n",
    "#     print(\n",
    "#         \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n\".format(\n",
    "#             test_loss,\n",
    "#             correct,\n",
    "#             len(test_loader.dataset),\n",
    "#             100.0 * correct / len(test_loader.dataset),\n",
    "#         )\n",
    "#     )\n",
    "#     return correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# project_abspath = os.path.dirname(os.getcwd())\n",
    "# print(project_abspath)\n",
    "\n",
    "# DATASET_NAME = 'mnist' # or 'cifar10'\n",
    "# DATA_ROOT = '/data/privacyGroup/liujunxu/datasets/{}'.format(DATASET_NAME)\n",
    "# RES_ROOT = project_abspath + '/results/sgd/{}'.format(DATASET_NAME)\n",
    "# if not os.path.exists(RES_ROOT):\n",
    "#     os.makedirs(RES_ROOT)\n",
    "    \n",
    "# IMAGENET_MEAN = {'mnist':[0.5], 'cifar10':[0.485, 0.456, 0.406]}\n",
    "# IMAGENET_STD = {'mnist':[0.5], 'cifar10':[0.229, 0.224, 0.225]}\n",
    "# if DATASET_NAME == 'mnist':\n",
    "#     train_data = MNIST(DATA_ROOT,\n",
    "#                     train=True,\n",
    "#                     download=True,\n",
    "#                     transform=Compose([ToTensor(), Normalize(IMAGENET_MEAN[DATASET_NAME], IMAGENET_STD[DATASET_NAME])]))\n",
    "#     test_data = MNIST(DATA_ROOT, \n",
    "#                   train=False, \n",
    "#                   download=True, \n",
    "#                   transform=Compose([ToTensor(), Normalize(IMAGENET_MEAN[DATASET_NAME], IMAGENET_STD[DATASET_NAME])]))\n",
    "\n",
    "# elif DATASET_NAME == 'cifar10':\n",
    "#     train_data = CIFAR10(DATA_ROOT, \n",
    "#                     train=True, \n",
    "#                     download=True, \n",
    "#                     transform=Compose([ToTensor(), Normalize(IMAGENET_MEAN[DATASET_NAME], IMAGENET_STD[DATASET_NAME])]))\n",
    "#     test_data = CIFAR10(DATA_ROOT, \n",
    "#                   train=False, \n",
    "#                   download=True, \n",
    "#                   transform=Compose([ToTensor(), Normalize(IMAGENET_MEAN[DATASET_NAME], IMAGENET_STD[DATASET_NAME])]))\n",
    "    \n",
    "# kwargs = {\"num_workers\": 1, \"pin_memory\": True}\n",
    "# device = 'cuda:2'\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     train_data,\n",
    "#     batch_size=60000, # use all data points\n",
    "#     shuffle=False,\n",
    "#     **kwargs,\n",
    "# )\n",
    "# test_loader = DataLoader(\n",
    "#     test_data,\n",
    "#     batch_size=1024,\n",
    "#     shuffle=True,\n",
    "#     **kwargs,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SampleConvNet(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 16, 8, 2, padding=3)\n",
    "#         self.conv2 = nn.Conv2d(16, 32, 4, 2)\n",
    "#         self.fc1 = nn.Linear(32 * 4 * 4, 32)\n",
    "#         self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # x of shape [B, 1, 28, 28]\n",
    "#         x = F.relu(self.conv1(x))   # -> [B, 16, 14, 14]\n",
    "#         x = F.max_pool2d(x, 2, 1)   # -> [B, 16, 13, 13]\n",
    "#         x = F.relu(self.conv2(x))   # -> [B, 32, 5, 5]\n",
    "#         x = F.max_pool2d(x, 2, 1)   # -> [B, 32, 4, 4]\n",
    "#         x = x.view(-1, 32 * 4 * 4)  # -> [B, 512]\n",
    "#         x = F.relu(self.fc1(x))     # -> [B, 32]\n",
    "#         x = self.fc2(x)             # -> [B, 10]\n",
    "#         return x\n",
    "\n",
    "#     def name(self):\n",
    "#         return \"SampleConvNet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Uniform privacy budgets settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchdp import PrivacyEngine\n",
    "\n",
    "# epochs = 150\n",
    "# budget = 11200\n",
    "# lr = .2\n",
    "# sigma = 170\n",
    "# max_per_sample_grad_norm = 10\n",
    "# delta = 1e-5\n",
    "# disable_dp = False\n",
    "    \n",
    "# run_results = []\n",
    "# active_points = []\n",
    "# running_gradient_sq_norms = [0]\n",
    "\n",
    "# model = SampleConvNet().to(device)\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0)\n",
    "# if not disable_dp:\n",
    "#     privacy_engine = PrivacyEngine(\n",
    "#         model,\n",
    "#         batch_size=60000,\n",
    "#         sample_size=len(train_loader.dataset),\n",
    "#         alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 80)),\n",
    "#         noise_multiplier=sigma,\n",
    "#         max_grad_norm=max_per_sample_grad_norm,\n",
    "#         norm_sq_budget = budget,\n",
    "#         should_clip = True,\n",
    "#     )\n",
    "#     privacy_engine.attach(optimizer)\n",
    "\n",
    "# for epoch in range(1, epochs + 1):\n",
    "#     gradient_norms = train(model, device, train_loader, optimizer, epoch, running_gradient_sq_norms[-1])\n",
    "\n",
    "#     # update running squared grad norms\n",
    "#     running_gradient_sq_norms.append(running_gradient_sq_norms[-1] + gradient_norms)\n",
    "\n",
    "#     # add new test accuracy \n",
    "#     run_results.append(test(model, device, test_loader))\n",
    "#     active_points.append(torch.sum(running_gradient_sq_norms[-1] < budget).cpu().numpy())\n",
    "#     if torch.sum(running_gradient_sq_norms[-1] - budget) == 0:\n",
    "#         break\n",
    "\t   \n",
    "# alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64))\n",
    "# eps_val = min([alpha/2*budget/(sigma**2 * max_per_sample_grad_norm**2) + np.log(1/delta)/(alpha-1) for alpha in alphas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# path1 = \"accuracies.pdf\"\n",
    "# path2 = \"active_points.pdf\"\n",
    "\n",
    "# plt.plot(active_points)\n",
    "# plt.axvline(int(budget/(max_per_sample_grad_norm**2)), 0, 1, c='m', linewidth = 3)\n",
    "# plt.ylim(0,65000)\n",
    "# plt.title(f\"ε = {eps_val:.1f}, δ =1e-5\", fontsize = 22)\n",
    "# plt.ylabel(f\"Number of active points\", fontsize = 22)\n",
    "# plt.xlabel(\"Step\", fontsize = 22)\n",
    "# plt.savefig(path2, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.clf()\n",
    "    \n",
    "# plt.plot(range(1,len(run_results) + 1), run_results, linewidth = 3)\n",
    "# plt.axvline(int(budget/(max_per_sample_grad_norm**2)), 0, 1, c='m', linewidth = 3)\n",
    "# plt.xlabel(\"Step\", fontsize = 22)\n",
    "# plt.ylabel(\"Test accuracy\", fontsize = 22)\n",
    "# plt.title(f\"ε = {eps_val:.1f}, δ =1e-5\", fontsize = 22)\n",
    "# plt.yticks([0.8, 0.825, 0.85, 0.875, 0.9, 0.925, 0.95, 0.975, 1.0])\n",
    "# plt.xticks(fontsize=16)\n",
    "# plt.yticks(fontsize=16)\n",
    "# plt.ylim(0.875,0.975)\n",
    "# plt.xlim(0,epochs)\n",
    "# plt.grid(True)\n",
    "# plt.savefig(path1, bbox_inches='tight')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Varied privacy budgets settings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 150\n",
    "# budgets = np.array([11200]*40000 + [14400]*10000 + [19800]*10000)\n",
    "# lr = .2\n",
    "# sigma = 170\n",
    "# max_per_sample_grad_norm = 10\n",
    "# delta = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchdp import PrivacyEngine\n",
    "\n",
    "# disable_dp = False\n",
    "\n",
    "# run_results = []\n",
    "# active_points = []\n",
    "# running_gradient_sq_norms = [0]\n",
    "\n",
    "# model = SampleConvNet().to(device)\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0)\n",
    "# if not disable_dp:\n",
    "#     privacy_engine = PrivacyEngine(\n",
    "#         model,\n",
    "#         batch_size=60000,\n",
    "#         sample_size=len(train_loader.dataset),\n",
    "#         alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 80)),\n",
    "#         noise_multiplier=sigma,\n",
    "#         max_grad_norm=max_per_sample_grad_norm,\n",
    "#         norm_sq_budget = budgets,\n",
    "#         should_clip = True,\n",
    "#     )\n",
    "#     privacy_engine.attach(optimizer)\n",
    "\n",
    "# for epoch in range(1, epochs + 1):\n",
    "#     gradient_norms = train(model, device, train_loader, optimizer, epoch, running_gradient_sq_norms[-1])\n",
    "\n",
    "#     # update running squared grad norms\n",
    "#     running_gradient_sq_norms.append(running_gradient_sq_norms[-1] + gradient_norms)\n",
    "\n",
    "#     # add new test accuracy \n",
    "#     run_results.append(test(model, device, test_loader))\n",
    "#     num_active_points = np.sum(running_gradient_sq_norms[-1].cpu().numpy() < np.array(budgets))\n",
    "#     active_points.append(num_active_points)\n",
    "#     if num_active_points == 0:\n",
    "#         break\n",
    "\t   \n",
    "# alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 64))\n",
    "# eps_val = min([alpha/2*budgets/(sigma**2 * max_per_sample_grad_norm**2) + np.log(1/delta)/(alpha-1) for alpha in alphas])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments on Medical dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fliter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fedrpdp.datasets.fed_heart_disease import (\n",
    "    Baseline,\n",
    "    BaselineLoss,\n",
    "    FedHeartDisease,\n",
    "    metric,\n",
    ")\n",
    "from fedrpdp.utils.flamby_utils import evaluate_model_on_tests\n",
    "\n",
    "metrics_dict = {\"AUC\": metric}\n",
    "use_gpu = True\n",
    "debug = False\n",
    "disable_dp = False\n",
    "\n",
    "budgets = np.array([0.15]*400 + [0.3]*50 + [0.45]*36)\n",
    "sigma = 100\n",
    "max_per_sample_grad_norm = 1.1\n",
    "delta = 1e-5\n",
    "\n",
    "num_workers_torch = 20\n",
    "device = 'cuda:0'\n",
    "NUM_EPOCHS_POOLED = 100\n",
    "LR = 0.05\n",
    "\n",
    "train_data = FedHeartDisease(train=True, pooled=True, debug=debug)\n",
    "test_data = FedHeartDisease(train=False, pooled=True, debug=debug)\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    num_workers=num_workers_torch,\n",
    "    batch_size=len(train_data),\n",
    "    shuffle=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    num_workers=num_workers_torch,\n",
    "    batch_size=len(test_data),\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rPDP-SGD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum budget: 0.04804357848189008 minimum budget: 0.012406216138705024\n",
      "[0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009 0.01  0.011 0.012\n",
      " 0.013 0.014 0.015 0.016 0.017 0.018 0.019 0.02  0.021 0.022 0.023 0.024\n",
      " 0.025 0.026 0.027 0.028 0.029 0.03  0.031 0.032 0.033 0.034 0.035 0.036\n",
      " 0.037 0.038 0.039 0.04  0.041 0.042 0.043 0.044 0.045 0.046 0.047 0.048\n",
      " 0.049 0.05  0.051 0.052 0.053 0.054 0.055 0.056 0.057 0.058 0.059 0.06\n",
      " 0.061 0.062 0.063 0.064 0.065 0.066 0.067 0.068 0.069 0.07  0.071 0.072\n",
      " 0.073 0.074 0.075 0.076 0.077 0.078 0.079 0.08  0.081 0.082 0.083 0.084\n",
      " 0.085 0.086 0.087 0.088 0.089 0.09  0.091 0.092 0.093 0.094 0.095 0.096\n",
      " 0.097 0.098 0.099 0.1   0.11  0.12  0.13  0.14  0.15  0.16  0.17  0.18\n",
      " 0.19  0.2   0.21  0.22  0.23  0.24  0.25  0.26  0.27  0.28  0.29  0.3\n",
      " 0.31  0.32  0.33  0.34  0.35  0.36  0.37  0.38  0.39  0.4   0.41  0.42\n",
      " 0.43  0.44  0.45  0.46  0.47  0.48  0.49  0.5   0.51  0.52  0.53  0.54\n",
      " 0.55  0.56  0.57  0.58  0.59  0.6   0.61  0.62  0.63  0.64  0.65  0.66\n",
      " 0.67  0.68  0.69  0.7   0.71  0.72  0.73  0.74  0.75  0.76  0.77  0.78\n",
      " 0.79  0.8   0.81  0.82  0.83  0.84  0.85  0.86  0.87  0.88  0.89  0.9\n",
      " 0.91  0.92  0.93  0.94  0.95  0.96  0.97  0.98  0.99  1.   ]\n",
      "[0.01240622 0.01240636 0.01240659 0.01240691 0.01240733 0.01240784\n",
      " 0.01240845 0.01240915 0.01240994 0.01241082 0.0124118  0.01241287\n",
      " 0.01241403 0.01241529 0.01241664 0.01241808 0.01241961 0.01242124\n",
      " 0.01242297 0.01242478 0.01242669 0.0124287  0.01243079 0.01243298\n",
      " 0.01243527 0.01243765 0.01244012 0.01244268 0.01244534 0.01244809\n",
      " 0.01245094 0.01245387 0.01245691 0.01246003 0.01246325 0.01246657\n",
      " 0.01246997 0.01247348 0.01247707 0.01248076 0.01248454 0.01248842\n",
      " 0.01249239 0.01249645 0.01250061 0.01250486 0.01250921 0.01251365\n",
      " 0.01251819 0.01252281 0.01252754 0.01253235 0.01253727 0.01254227\n",
      " 0.01254737 0.01255256 0.01255785 0.01256323 0.01256871 0.01257428\n",
      " 0.01257994 0.0125857  0.01259155 0.0125975  0.01260354 0.01260968\n",
      " 0.01261591 0.01262224 0.01262866 0.01263517 0.01264178 0.01264848\n",
      " 0.01265528 0.01266217 0.01266916 0.01267624 0.01268342 0.01269069\n",
      " 0.01269805 0.01270551 0.01271307 0.01272071 0.01272846 0.0127363\n",
      " 0.01274423 0.01275226 0.01276038 0.0127686  0.01277691 0.01278532\n",
      " 0.01279382 0.01280242 0.01281111 0.0128199  0.01282878 0.01283775\n",
      " 0.01284683 0.01285599 0.01286525 0.01287461 0.0129734  0.01308171\n",
      " 0.01319956 0.01332694 0.01346389 0.0136104  0.0137665  0.01393218\n",
      " 0.01410746 0.01429234 0.01448684 0.01469094 0.01490467 0.01512802\n",
      " 0.01536099 0.01560359 0.01585581 0.01611765 0.01638911 0.01667018\n",
      " 0.01696087 0.01726115 0.01757104 0.01789051 0.01821955 0.01855817\n",
      " 0.01890633 0.01926404 0.01963128 0.02000803 0.02039428 0.02079001\n",
      " 0.02119519 0.02160982 0.02203386 0.02246731 0.02291013 0.0233623\n",
      " 0.0238238  0.0242946  0.02477467 0.02526399 0.02573905 0.02621277\n",
      " 0.02669506 0.02718588 0.02765454 0.02812846 0.0286103  0.02910001\n",
      " 0.02956902 0.03004098 0.03052026 0.03100681 0.03148292 0.0319513\n",
      " 0.03242643 0.0329083  0.03339688 0.03386335 0.03433188 0.03480662\n",
      " 0.03528755 0.03577466 0.03624123 0.03670821 0.03718091 0.03765933\n",
      " 0.03814343 0.03861791 0.03908148 0.03955035 0.04002447 0.04050385\n",
      " 0.04098844 0.04145588 0.04191874 0.04238645 0.042859   0.04333636\n",
      " 0.04381851 0.04429065 0.04475105 0.04521592 0.04568524 0.04615899\n",
      " 0.04663714 0.04711969 0.04758457 0.04804358]\n",
      "r2 score of the curve fitting. 0.9946137016413485\n",
      "check the valid scope of privacy budget:  0.04804357848189008 0.012406216138705024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wamdm/miniconda3/envs/liujunxu_tf1/lib/python3.7/site-packages/autodp/rdp_acct.py:724: RuntimeWarning: divide by zero encountered in log\n",
      "  + (alpha+1-jvec[0:alpha-1])*np.log(1-prob))\n",
      "/home/wamdm/miniconda3/envs/liujunxu_tf1/lib/python3.7/site-packages/autodp/utils.py:8: RuntimeWarning: invalid value encountered in subtract\n",
      "  return a+np.log(np.sum(np.exp(x-a)))\n",
      "/home/wamdm/miniconda3/envs/liujunxu_tf1/lib/python3.7/site-packages/autodp/rdp_acct.py:725: RuntimeWarning: divide by zero encountered in log\n",
      "  results[alpha-1] = utils.stable_logsumexp_two((alpha-1)*np.log(1-prob)\n"
     ]
    }
   ],
   "source": [
    "from fedrpdp.autodp.analysis.privcost_utils import PrivCostEstimator\n",
    "MIN_BUDGET, MAX_BUDGET, VALID_BUDGET = 0.1, 10.0, 1.0\n",
    "gen_bounded_priv_bgts_func = lambda priv_bgts: np.array([min(max(x, MIN_BUDGET), MAX_BUDGET) for x in priv_bgts])\n",
    "\n",
    "privCostCurve = PrivCostEstimator(\n",
    "    sigma = sigma, \n",
    "    inner_iters = 1, \n",
    "    outer_iters = 0, \n",
    "    outer_ratio = 1.0,\n",
    "    delta = delta).estimator(est_object=\"probs\")\n",
    "\n",
    "sampling_probs = np.array([privCostCurve(eps) for eps in budgets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonUniformBatchSampler:\n",
    "    def __init__(self, dataset, probs):\n",
    "        self.length = len(dataset)\n",
    "        self.probs = probs\n",
    "       \n",
    "    def __iter__(self):      \n",
    "        indices = np.where(np.random.random(self.length) <= self.probs)[0]\n",
    "        yield indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.iterations\n",
    "\n",
    "def dp_train(model, device, train_loader, optimizer, epoch, disable_dp=False, delta=1e-5):\n",
    "    model.train()\n",
    "    criterion = BaselineLoss()\n",
    "    losses = []\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_data, \n",
    "        batch_sampler=NonUniformBatchSampler(train_data, sampling_probs),\n",
    "        shuffle=False\n",
    "    )\n",
    "    data, target = next(iter(train_loader))\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    output = model(data)\n",
    "\n",
    "    k, v = next(iter(metrics_dict.items()))\n",
    "    train_metric = v(\n",
    "        target.detach().cpu().numpy(), output.detach().cpu().numpy()\n",
    "    )\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    if not disable_dp:\n",
    "        # Note that we only show the 1st point's cumulative privacy cost.\n",
    "        epsilon_1, best_alpha_1 = optimizer.privacy_engine.get_privacy_spent(0, delta)\n",
    "        epsilon_2, best_alpha_2 = optimizer.privacy_engine.get_privacy_spent(400, delta)\n",
    "        epsilon_3, best_alpha_3 = optimizer.privacy_engine.get_privacy_spent(450, delta)\n",
    "        print(\n",
    "            f\"Train Epoch: {epoch} \\t\"\n",
    "            f\"Loss: {np.mean(losses):.6f} \"\n",
    "            f\"{k}: {train_metric:.6f} \"\n",
    "            f\"δ: {delta} \"\n",
    "            f\"ε1 = {epsilon_1:.2f} for α1 = {best_alpha_1}, \"\n",
    "            f\"ε2 = {epsilon_2:.2f} for α2 = {best_alpha_2}, \"\n",
    "            f\"ε3 = {epsilon_3:.2f} for α3 = {best_alpha_3}. \"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"Train Epoch: {epoch} \\t Loss: {np.mean(losses):.6f}\")\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    criterion = BaselineLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        data, target = next(iter(test_loader))\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss = criterion(output, target).item()\n",
    "        \n",
    "        for k, v in metrics_dict.items():\n",
    "            test_metric = v(\n",
    "                target.detach().cpu().numpy(), output.detach().cpu().numpy()\n",
    "            )\n",
    "        print(\n",
    "            \"\\nTest set: Average loss: {:.4f}, {}: {:.2f}\\n\".format(\n",
    "                test_loss,\n",
    "                k,\n",
    "                test_metric,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return test_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "dp_step() missing 1 required positional argument: 'running_norms'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_54556/2385075023.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS_POOLED\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mdp_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;31m# add new test accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mrun_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_54556/3135377683.py\u001b[0m in \u001b[0;36mdp_train\u001b[0;34m(model, device, train_loader, optimizer, epoch, disable_dp, delta)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: dp_step() missing 1 required positional argument: 'running_norms'"
     ]
    }
   ],
   "source": [
    "from torchdp import PrivacyEngine\n",
    "\n",
    "run_results = []\n",
    "model = Baseline().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=LR, momentum=0)\n",
    "\n",
    "def generate_rdp_orders():\n",
    "    dense = 1.07\n",
    "    alpha_list = [int(dense ** i + 1) for i in range(int(math.floor(math.log(1000, dense))) + 1)]\n",
    "    alpha_list = np.unique(alpha_list)\n",
    "    return alpha_list\n",
    "\n",
    "privacy_engine = PrivacyEngine(\n",
    "    model,\n",
    "    batch_size=len(train_loader.dataset),\n",
    "    sample_size=len(train_loader.dataset),\n",
    "    # alphas=[1 + x / 10.0 for x in range(1, 100)] + list(range(12, 80)),\n",
    "    alphas = generate_rdp_orders(),\n",
    "    noise_multiplier=sigma,\n",
    "    max_grad_norm=max_per_sample_grad_norm,\n",
    "    norm_sq_budget = budgets,\n",
    "    should_clip = True,\n",
    ")\n",
    "privacy_engine.attach(optimizer)\n",
    "\n",
    "# pdp_optim = pdp_optim_cls(torch.optim.SGD)\n",
    "# optimizer = pdp_optim(\n",
    "#     l2_norm_clip=max_per_sample_grad_norm,\n",
    "#     gauss_std=sigma * max_per_sample_grad_norm,\n",
    "#     minibatch_size=1,\n",
    "#     microbatch_size=len(train_data),\n",
    "#     params=model.parameters(),\n",
    "#     lr=LR)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS_POOLED + 1):\n",
    "    dp_train(model, device, train_loader, optimizer, epoch)\n",
    "    # add new test accuracy \n",
    "    run_results.append(test(model, device, test_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "9cd2817a37aac10ac04dd4bd211d68db65c45a714a600fc3dbad943ddf1ad11a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
